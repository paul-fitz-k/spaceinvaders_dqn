{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac67d84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary packages:\n",
    "\n",
    "import gym \n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "import numpy as np\n",
    "from cpprb import PrioritizedReplayBuffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c46857d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# have a look at the environment:\n",
    "e = gym.make('ALE/SpaceInvaders-ram-v5')\n",
    "e.reset()\n",
    "e.step(0)\n",
    "e.step(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da971b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create wrapper to make it easier to interact with the environment:\n",
    "\n",
    "class wrapper():\n",
    "    def __init__(self, envName=\"ALE/SpaceInvaders-ram-v5\"):\n",
    "        self.env = gym.make(envName)\n",
    "        self.state = None\n",
    "        self.last_lives = 0\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = self.env.reset()\n",
    "        self.last_lives = 0\n",
    "        terminal_life_lost = True \n",
    "        \n",
    "        return terminal_life_lost\n",
    "\n",
    "    def step(self,action):\n",
    "        new_ram, reward, terminal, info = self.env.step(action)\n",
    "            \n",
    "        if info['lives'] < self.last_lives:\n",
    "            terminal_life_lost = True\n",
    "        else:\n",
    "            terminal_life_lost = terminal\n",
    "        self.last_lives = info['lives']\n",
    "\n",
    "        self.state = new_ram\n",
    "        \n",
    "        return new_ram, reward, terminal, terminal_life_lost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99888a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a class for our replay buffer using Prioritized Experience Replay from the cpprb package:\n",
    "\n",
    "class ExperienceReplay():\n",
    "    \"\"\"Making use of the cpprb package to implement PER\n",
    "        with 100,000 as the memory size\"\"\"\n",
    "    def __init__(self, size=100000, ram=128, batch_size=32):\n",
    "        self.size = size\n",
    "        self.ram = ram\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.prb = PrioritizedReplayBuffer(size,\n",
    "                              {\"obs\": {\"shape\": (ram)},\n",
    "                               \"act\": {},\n",
    "                               \"rew\": {},\n",
    "                               \"next_obs\": {\"shape\": (ram)},\n",
    "                               \"done\": {}},\n",
    "                              alpha=0.5)\n",
    "        \n",
    "    def add_experience(self, action, ram, reward, new_ram, terminal):\n",
    "        self.prb.add(obs=ram,\n",
    "            act=action,\n",
    "            rew=reward,\n",
    "            next_obs=new_ram,\n",
    "            done=terminal)\n",
    "            \n",
    "    def get_minibatch(self):\n",
    "        s = self.prb.sample(self.batch_size)\n",
    "        act = np.array([i[0] for i in s['act']])\n",
    "        rew = np.array([i[0] for i in s['rew']])\n",
    "        done = np.array([i[0] for i in s['done']])\n",
    "        return s['obs'], act, rew, s['next_obs'], done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6924f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a class for decreasing the exploration probability over time:\n",
    "\n",
    "class Exploration():\n",
    "    def __init__(self, DQN, n_actions, eps_init=1.0, eps_f=0.01, eps_anneal=2000000):\n",
    "\n",
    "        self.n_actions = n_actions\n",
    "        self.eps_anneal = eps_anneal\n",
    "        self.eps_f = eps_f\n",
    "        self.linspace = np.linspace(eps_init, eps_f, eps_anneal)\n",
    "        self.DQN = DQN\n",
    "        self.playing = False\n",
    "\n",
    "    def get_action(self, session, frame_number, state):\n",
    "        \n",
    "        if frame_number <= 10000:\n",
    "            eps = self.linspace[0]\n",
    "        elif (frame_number > 10000) and (frame_number < self.eps_anneal):\n",
    "            eps = self.linspace[frame_number-10000]\n",
    "        elif self.playing == True:\n",
    "            eps = 0.0\n",
    "        else:\n",
    "            eps = self.eps_f\n",
    "        \n",
    "        if np.random.rand(1) < eps:\n",
    "            return np.random.randint(0, self.n_actions)\n",
    "        return session.run(self.DQN.best_action, feed_dict={self.DQN.input:[state]})[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088d72df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create class for our Dueling DQN agent:\n",
    "\n",
    "class DQN_agent():   \n",
    "    def __init__(self, n_actions, learning_rate=0.00025, \n",
    "                 ram_size=128):\n",
    "\n",
    "        self.n_actions = n_actions\n",
    "        self.learning_rate = learning_rate\n",
    "        self.ram_size = ram_size\n",
    "        \n",
    "        self.input = tf.placeholder(shape=[None, self.ram_size], \n",
    "                                    dtype=tf.float32)\n",
    "        self.inputscaled = self.input/255\n",
    "        \n",
    "        self.dense1 = tf.layers.dense(\n",
    "            inputs=self.inputscaled, units=512, kernel_initializer=tf.variance_scaling_initializer(scale=2),\n",
    "             activation=tf.nn.relu, name='dense1')\n",
    "        self.dense2 = tf.layers.dense(\n",
    "            inputs=self.dense1, units=256, kernel_initializer=tf.variance_scaling_initializer(scale=2),\n",
    "             activation=tf.nn.relu, name='dense2')\n",
    "        self.dense3 = tf.layers.dense(\n",
    "            inputs=self.dense2, units=self.ram_size, kernel_initializer=tf.variance_scaling_initializer(scale=2),\n",
    "             activation=tf.nn.relu, name='dense3')\n",
    "        self.valuestream, self.advantagestream = tf.split(self.dense3,2,axis=1)\n",
    "        self.valuestream = tf.layers.flatten(self.valuestream)\n",
    "        self.advantagestream = tf.layers.flatten(self.advantagestream)\n",
    "        self.advantage = tf.layers.dense(\n",
    "            inputs=self.advantagestream, units=self.n_actions,\n",
    "            kernel_initializer=tf.variance_scaling_initializer(scale=2), name=\"advantage\")\n",
    "        self.value = tf.layers.dense(\n",
    "            inputs=self.valuestream, units=1, \n",
    "            kernel_initializer=tf.variance_scaling_initializer(scale=2), name='value')\n",
    "\n",
    "        self.q_values = self.value + tf.subtract(self.advantage, tf.reduce_mean(self.advantage, axis=1, keepdims=True))\n",
    "        self.best_action = tf.argmax(self.q_values,1)\n",
    "        self.target_q = tf.placeholder(shape=[None], dtype=tf.float32)\n",
    "        self.action = tf.placeholder(shape=[None], dtype=tf.int32)\n",
    "        self.Q = tf.reduce_sum(tf.multiply(self.q_values, tf.one_hot(self.action, self.n_actions, dtype=tf.float32)), axis=1)\n",
    "\n",
    "        self.loss = tf.reduce_mean(tf.losses.huber_loss(labels=self.target_q, predictions=self.Q))\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate)\n",
    "        self.update = self.optimizer.minimize(self.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d4e3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to activate learning in the training process:\n",
    "\n",
    "def learn(session, replay_memory, main_dqn, target_dqn, batch_size=32, gamma=0.99):\n",
    "\n",
    "    # Draw a minibatch from the replay memory\n",
    "    minibatch = replay_memory.get_minibatch()\n",
    "\n",
    "    arg_q_max = session.run(main_dqn.best_action, feed_dict={main_dqn.input:minibatch[3]})\n",
    "    q_vals = session.run(target_dqn.q_values, feed_dict={target_dqn.input:minibatch[3]})\n",
    "\n",
    "    double_q = q_vals[range(batch_size), arg_q_max]\n",
    "    target_q = minibatch[2] + (gamma*double_q*(1-minibatch[4]))\n",
    "\n",
    "    loss, update = session.run([main_dqn.loss,main_dqn.update],feed_dict={main_dqn.input:minibatch[0],main_dqn.target_q:target_q,main_dqn.action:minibatch[1]})\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f2fd4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class to update the target network every number of steps:\n",
    "\n",
    "class Update_Target():\n",
    "    def __init__(self, main_variables, target_variables):\n",
    "        self.main_variables = main_variables\n",
    "        self.target_variables = target_variables\n",
    "\n",
    "    def update_target_variables(self):\n",
    "        update_ops = []\n",
    "        for i, var in enumerate(self.main_variables):\n",
    "            copy_op = self.target_variables[i].assign(var.value())\n",
    "            update_ops.append(copy_op)\n",
    "        return update_ops\n",
    "            \n",
    "    def __call__(self, sess):\n",
    "        update_ops = self.update_target_variables()\n",
    "        for copy_op in update_ops:\n",
    "            sess.run(copy_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dbd0169",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to clip rewards, to avoid large updates due to large magnitudes of rewards:\n",
    "\n",
    "def clip_reward(reward):\n",
    "    return int(np.sign(reward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb1120f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final definitions:\n",
    "\n",
    "tf.compat.v1.reset_default_graph()\n",
    "\n",
    "# Defining the environment:\n",
    "atari = wrapper()\n",
    "\n",
    "# Defining the networks:\n",
    "with tf.variable_scope('main'):\n",
    "    main = DQN_agent(atari.env.action_space.n)\n",
    "with tf.variable_scope('target'):\n",
    "    target = DQN_agent(atari.env.action_space.n)\n",
    "\n",
    "# initialise variables and call saver to save our model throughout training:\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()    \n",
    "main_vs = tf.trainable_variables(scope='main')\n",
    "target_vs = tf.trainable_variables(scope='target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77103253",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now to train the model:\n",
    "\n",
    "memory = ExperienceReplay()\n",
    "update_networks = Update_Target(main_vs, target_vs)\n",
    "explore = Exploration(main, atari.env.action_space.n)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    try:\n",
    "        saver = tf.train.import_meta_graph(\"tmp/spaceinvaders_dueldqn-600.meta\")\n",
    "        saver.restore(sess,tf.train.latest_checkpoint(\"tmp/\"))\n",
    "    except:\n",
    "        print('no previous training!')\n",
    "    sess.run(init)\n",
    "    \n",
    "    frame_number = 0\n",
    "    rewards = []\n",
    "    loss_list = []\n",
    "    \n",
    "    while frame_number < 10000000:\n",
    "\n",
    "        terminal_life_lost = atari.reset()\n",
    "        episode_reward_sum = 0\n",
    "        for _ in range(20000):\n",
    "            action = explore.get_action(sess, frame_number, atari.state)   \n",
    "            s = atari.state\n",
    "            new_ram, reward, terminal, terminal_life_lost = atari.step(action)  \n",
    "            frame_number += 1\n",
    "            episode_reward_sum += reward\n",
    "            clipped_reward = clip_reward(reward)\n",
    "            memory.add_experience(action=action,ram=s,new_ram=new_ram,reward=clipped_reward,terminal=terminal_life_lost)   \n",
    "            \n",
    "            # 10000 random actions before learning:\n",
    "            if frame_number > 10000:\n",
    "                loss = learn(sess, memory, main, target)\n",
    "                loss_list.append(loss)\n",
    "\n",
    "            # update target network every 5000 frames:\n",
    "            if frame_number % 5000 == 0 and frame_number > 10000:\n",
    "                update_networks(sess)\n",
    "                \n",
    "            if terminal:\n",
    "                terminal = False\n",
    "                break\n",
    "\n",
    "        rewards.append(episode_reward_sum)\n",
    "\n",
    "        # save network and print rewards every 100 episodes\n",
    "        if len(rewards) % 100 == 0:\n",
    "            saver.save(sess, 'tmp/spaceinvaders_dueldqn', global_step=len(rewards))\n",
    "            print(rewards[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d64fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae05ddef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a5ed86",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c560376",
   "metadata": {},
   "outputs": [],
   "source": [
    "means2 = []\n",
    "m=0\n",
    "for i in range(len(rewards)):\n",
    "    m += rewards[i]\n",
    "    if i%100 == 0:\n",
    "        means2.append(m/100)\n",
    "        m=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44adb7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.title('Rolling Means of Returns from 16,000 Episodes')\n",
    "plt.xlabel('Epoch (each epoch contains 100 episodes)')\n",
    "plt.ylabel('Mean Episodic Reward')\n",
    "plt.plot(means2[:-2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5112b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(means2).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c17f9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "means2[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca4a8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "means2[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09cb54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(rewards).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51720e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_means = pd.DataFrame(means2, columns=['Rolling Means'])\n",
    "df_rewards = pd.DataFrame(rewards, columns=['Rewards'])\n",
    "df_means.to_csv('means2.csv', index=False)\n",
    "df_rewards.to_csv('rewards2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd44455",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluating the performance of the agent. i.e. seeing him play the game:\n",
    "\n",
    "explore = Exploration(main, atari.env.action_space.n)\n",
    "explore.playing = True\n",
    "with tf.Session() as sess:\n",
    "    saver = tf.train.import_meta_graph(\"tmp/spaceinvaders_dueldqn-15700.meta\")\n",
    "    saver.restore(sess,tf.train.latest_checkpoint(\"tmp/\"))\n",
    "    environment = gym.make('ALE/SpaceInvaders-ram-v5', render_mode='human')\n",
    "    state = environment.reset()\n",
    "    done = False\n",
    "    reward_sum = 0\n",
    "    while not done:\n",
    "        action = explore.get_action(sess, 3000000, state)\n",
    "        new_state, reward, terminal, _ = environment.step(action)\n",
    "        reward_sum += reward\n",
    "        state = new_state\n",
    "        done = terminal\n",
    "    environment.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e5c773",
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d71be7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
