{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac67d84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym \n",
    "import random\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import Huber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67857d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cols = ['episode','steps','reward','epsilon','gamma','alpha','learning_rate']\n",
    "#df_train_DQN_agent_experience_replay_target_network_SpaceInvaders_ram = pd.DataFrame(data=None, columns=cols)\n",
    "#df_train_DQN_agent_experience_replay_target_network_SpaceInvaders_ram.to_csv('df_DQN_agent_experience_replay_target_network_SpaceInvaders_ram.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd34106b",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = \"ALE/SpaceInvaders-ram-v5\"\n",
    "env = gym.make(env_name)\n",
    "input_shape = env.observation_space.shape\n",
    "actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9944e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN_agent_experience_replay_target_network():\n",
    "    def __init__(self):\n",
    "        self.ready = False # tells us if model is ready to train\n",
    "        self.epsilon_max = 1 #can change these values depending on the game and the number of states, and scarcity/abundance of reward\n",
    "        self.epsilon_min = 0.065\n",
    "        self.gamma = 0.9\n",
    "        self.alpha = 0.2\n",
    "        self.mini_batch_size = 32*2\n",
    "        self.counter = 0 # update weights of network2 every x steps!\n",
    "        self.replay_memory = [] # as this re-initialises every time the agent is initialised it means he can only learn from past episodes if we run a number of episodes consecutively in one cell execution\n",
    "        self.lr = 0.001\n",
    "        self.optimizer = Adam(learning_rate=self.lr)\n",
    "        self.memory_limit = 20000\n",
    "\n",
    "\n",
    "        try:\n",
    "            self.network1 = tf.keras.models.load_model('model_DQN_agent_experience_replay_target_network_SpaceInvaders_ram',compile=True)\n",
    "        except:\n",
    "            print('No previous history of training') \n",
    "            \n",
    "            self.network1 = Sequential()\n",
    "            self.network1.add(BatchNormalization())\n",
    "            self.network1.add(Dense(512, activation='relu',input_shape = input_shape))\n",
    "            self.network1.add(BatchNormalization())\n",
    "            self.network1.add(Dense(256, activation='relu'))\n",
    "            self.network1.add(BatchNormalization())\n",
    "            self.network1.add(Dense(64, activation='relu'))\n",
    "            self.network1.add(BatchNormalization())\n",
    "            self.network1.add(Dense(actions, activation='linear'))\n",
    "            self.network1.compile(optimizer=self.optimizer, loss=Huber(), metrics=['accuracy'])\n",
    "\n",
    "        self.network2 = self.network1\n",
    "\n",
    "\n",
    "    def choose_action(self,state,epsilon_upper,decay_step):\n",
    "        \"\"\"use neural network to decide best action\"\"\"\n",
    "        self.counter += 1\n",
    "        epsilon = epsilon_upper - (0.02 *(1- np.exp(-decay_step/100))) # lowest exploration prob will ever get is approx. 0.005\n",
    "        if (np.random.uniform() < epsilon):\n",
    "            action_index = np.random.randint(0,actions)\n",
    "        else:\n",
    "            action_index = np.argmax(self.network1.predict(np.expand_dims(state,axis=0).astype('float32'))) \n",
    "        return action_index\n",
    "        \n",
    "\n",
    "    def memorise(self,state,action,reward,new_state,done):\n",
    "        \"\"\"Store each transition in memory\"\"\"\n",
    "        self.replay_memory.append((state,action,reward,new_state,done))\n",
    "        if len(self.replay_memory)>self.memory_limit:\n",
    "            self.replay_memory.remove(self.replay_memory[0])\n",
    "\n",
    "    def compute_TD_target_and_learn(self):\n",
    "        \"\"\"Use network2 to simulate target value (y_true)\"\"\"\n",
    "        if self.counter<=self.mini_batch_size:\n",
    "            print('Not ready to train yet')\n",
    "            return\n",
    "        else:\n",
    "            mini_batch = random.sample(self.replay_memory,self.mini_batch_size)\n",
    "            self.new_mini_batch = pd.DataFrame(data=mini_batch,columns=['state','action','reward','new_state','done'])\n",
    "            self.dones = self.new_mini_batch[\"done\"].astype(int)\n",
    "            self.rewards = np.array(self.new_mini_batch['reward'])\n",
    "            self.new_states = list(self.new_mini_batch['new_state'])\n",
    "            self.new_states = np.asarray(self.new_states).astype('float32')\n",
    "            self.preds = np.max(self.network2.predict(self.new_states)) #1\n",
    "            self.targets = self.rewards + self.dones*(self.gamma * np.array(self.preds))\n",
    "            self.actions = np.asarray(self.new_mini_batch['action']).astype(int)\n",
    "            self.states = list(self.new_mini_batch['state'])\n",
    "            self.states = np.asarray(self.states).astype('float32')\n",
    "            self.outputs = self.network1.predict(self.states)\n",
    "            self.outputs[range(len(self.actions)),self.actions] = (1-self.alpha)*self.outputs[range(len(self.actions)),self.actions] + (self.alpha*self.targets)            \n",
    "            self.network1.fit(self.states,np.asarray(self.outputs).astype('float32'),batch_size = self.mini_batch_size,verbose=0)\n",
    "            if self.counter % 50 == 0:        \n",
    "                self.network2 = self.network1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77103253",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_DQN_agent_experience_replay_target_network_SpaceInvaders_ram = pd.read_csv('df_DQN_agent_experience_replay_target_network_SpaceInvaders_ram.csv')\n",
    "history = df_DQN_agent_experience_replay_target_network_SpaceInvaders_ram.values.tolist()\n",
    "agent = DQN_agent_experience_replay_target_network()\n",
    "episodes = 1\n",
    "for episode in range(1, episodes+1):\n",
    "    if len(history)>0:\n",
    "        episode_number = history[-1][0]+1\n",
    "    else:\n",
    "        episode_number = 1\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    score = 0 \n",
    "    decay_step = 0\n",
    "    epsilon_upper = max(agent.epsilon_max - (((episode_number/10)-1)*0.02),agent.epsilon_min) # for each episode, exploration rate will decay between these bounds\n",
    "    while not done:\n",
    "        action = agent.choose_action(state,epsilon_upper,decay_step)\n",
    "        decay_step += 1\n",
    "        n_state, reward, done, info = env.step(action)\n",
    "        score+=reward\n",
    "        agent.memorise(state,action,reward,n_state,done)\n",
    "        agent.compute_TD_target_and_learn()\n",
    "        state = n_state\n",
    "    print('Episode:{} Steps:{} Score:{}'.format(episode_number,decay_step, score))\n",
    "    history.append((episode_number,decay_step,score,epsilon_upper - (0.02 *(1- np.exp(-decay_step/100))),agent.gamma,agent.alpha,agent.lr,1,50)) # to keep track of overall stats\n",
    "    agent.network1.save('model_DQN_agent_experience_replay_target_network_SpaceInvaders_ram')\n",
    "    df_DQN_agent_experience_replay_target_network_SpaceInvaders_ram = pd.DataFrame(data=history, columns=['episode','steps','reward','epsilon','gamma','alpha','learning_rate','weights','target_update_frequency'])\n",
    "    df_DQN_agent_experience_replay_target_network_SpaceInvaders_ram.to_csv('df_DQN_agent_experience_replay_target_network_SpaceInvaders_ram.csv',index=False)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4af600d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Max raw score in single episode\n",
    "\n",
    "max(df_DQN_agent_experience_replay_target_network_SpaceInvaders_ram['reward'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f1e6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find moving average of reward\n",
    "\n",
    "plt.plot(range(len(np.convolve(df_DQN_agent_experience_replay_target_network_SpaceInvaders_ram['reward'][:850], np.ones(100)/100, mode='valid'))),np.convolve(df_DQN_agent_experience_replay_target_network_SpaceInvaders_ram['reward'][:850], np.ones(100)/100, mode='valid'))\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Average Reward')\n",
    "plt.title('DQN with Target Network and Replay Experience Episodes vs Moving Average Reward')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
